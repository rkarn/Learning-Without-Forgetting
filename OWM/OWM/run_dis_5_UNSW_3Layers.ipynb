{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training csv file.\n",
      "Reading Testing csv file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir('/Users/rupesh.karn/Desktop/WorkPart-1/UNSW-NB15 Dataset')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (16.0, 5.0)\n",
    "\n",
    "# Read in the training CSV file\n",
    "print(\"Reading Training csv file.\")\n",
    "df1 = pd.read_csv(\"/root/pathint/fig_split_mnist/UNSW_NB15_training-set.csv\")\n",
    "df1.drop('label', axis=1, inplace=True)\n",
    "\n",
    "obj_df=df1\n",
    "\n",
    "obj_df[\"proto\"] = obj_df[\"proto\"].astype('category')\n",
    "obj_df[\"service\"] = obj_df[\"service\"].astype('category')\n",
    "obj_df[\"state\"] = obj_df[\"state\"].astype('category')\n",
    "obj_df[\"proto_cat\"] = obj_df[\"proto\"].cat.codes\n",
    "obj_df[\"service_cat\"] = obj_df[\"service\"].cat.codes\n",
    "obj_df[\"state_cat\"] = obj_df[\"state\"].cat.codes\n",
    "\n",
    "obj_df[\"proto\"] = obj_df[\"proto_cat\"]\n",
    "obj_df[\"service\"] = obj_df[\"service_cat\"]\n",
    "obj_df[\"state\"] = obj_df[\"state_cat\"]\n",
    "\n",
    "obj_df.drop('proto_cat', axis=1, inplace=True)\n",
    "obj_df.drop('service_cat', axis=1, inplace=True)\n",
    "obj_df.drop('state_cat', axis=1, inplace=True)\n",
    "\n",
    "Y_train_all_attacks = obj_df[\"attack_cat\"]\n",
    "obj_df=pd.get_dummies(obj_df, columns=[\"attack_cat\"])\n",
    "\n",
    "\n",
    "X_train = obj_df.values[:,:-10]\n",
    "Y_train_onehot_encoded = obj_df.values[:,-10:]\n",
    "\n",
    "for j in range(0,43):\n",
    "    maximum = max(X_train[:,j])\n",
    "    for i in range(0,len(X_train)):\n",
    "        X_train[i,j] = round(X_train[i,j]/maximum,3)\n",
    "\n",
    "# Read in the testing CSV file \n",
    "print(\"Reading Testing csv file.\")\n",
    "df2 = pd.read_csv(\"/root/pathint/fig_split_mnist/UNSW_NB15_testing-set.csv\")\n",
    "df2.drop('label', axis=1, inplace=True)\n",
    "\n",
    "obj_df2=df2\n",
    "\n",
    "obj_df2[\"proto\"] = obj_df2[\"proto\"].astype('category')\n",
    "obj_df2[\"service\"] = obj_df2[\"service\"].astype('category')\n",
    "obj_df2[\"state\"] = obj_df2[\"state\"].astype('category')\n",
    "obj_df2[\"proto_cat\"] = obj_df2[\"proto\"].cat.codes\n",
    "obj_df2[\"service_cat\"] = obj_df2[\"service\"].cat.codes\n",
    "obj_df2[\"state_cat\"] = obj_df2[\"state\"].cat.codes\n",
    "\n",
    "obj_df2[\"proto\"] = obj_df2[\"proto_cat\"]\n",
    "obj_df2[\"service\"] = obj_df2[\"service_cat\"]\n",
    "obj_df2[\"state\"] = obj_df2[\"state_cat\"]\n",
    "\n",
    "obj_df2.drop('proto_cat', axis=1, inplace=True)\n",
    "obj_df2.drop('service_cat', axis=1, inplace=True)\n",
    "obj_df2.drop('state_cat', axis=1, inplace=True)\n",
    "\n",
    "Y_test_all_attacks = obj_df2[\"attack_cat\"]\n",
    "obj_df2=pd.get_dummies(obj_df2, columns=[\"attack_cat\"])\n",
    "\n",
    "\n",
    "X_test = obj_df2.values[:,:-10]\n",
    "Y_test_onehot_encoded = obj_df2.values[:,-10:]\n",
    "\n",
    "for j in range(0,43):\n",
    "    maximum = max(X_train[:,j])\n",
    "    for i in range(0,len(X_test)):\n",
    "        X_test[i,j] = round(X_test[i,j]/maximum,3)\n",
    "\n",
    "\n",
    "estimators_number = list(range(10,30))\n",
    "\n",
    "dataspace = 0;\n",
    "overall_accuracy_matrix = [None]*len(X_train)\n",
    "iTERATION=0\n",
    "dataspace_number=1\n",
    "attack_type = 4\n",
    "Y_train = obj_df.values[:,-attack_type]\n",
    "Y_test = obj_df2.values[:,-attack_type]\n",
    "\n",
    "cleanup_nums = {\"Worms\":0, \"Shellcode\":1, \"Reconnaissance\":2, \"Normal\":3, \"Generic\":4, \"Fuzzers\":5, \"Exploits\":6, \"DoS\":7, \"Backdoor\":8, \"Analysis\":9}\n",
    "Y_train_all_attacks.replace(cleanup_nums,inplace=True)\n",
    "Y_test_all_attacks.replace(cleanup_nums,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 size: Trainset - (24933, 43), (24933, 10), Testset - (58184, 43), (58184, 10)\n",
      "Task 1 size: Trainset - (1260, 43), (1260, 10), Testset - (3746, 43), (3746, 10)\n",
      "Task 2 size: Trainset - (15221, 43), (15221, 10), Testset - (45657, 43), (45657, 10)\n",
      "Task 3 size: Trainset - (422, 43), (422, 10), Testset - (1263, 43), (1263, 10)\n",
      "Task 4 size: Trainset - (40496, 43), (40496, 10), Testset - (66491, 43), (66491, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "#task_labels = [[0,1], [2,3], [4,5], [6,7], [8,9],[1,5],[7,9],[3,8],[0,6],[4,2]]\n",
    "#task_labels = [[4,2], [0,6], [3,8], [9,7], [1,5],[8,9],[6,7],[5,5],[3,2],[0,1]]\n",
    "#task_labels = [[8,9], [6,7], [4,5], [2,3], [0,1]]\n",
    "#task_labels = [[0,9], [7,8], [3,6], [1,4], [2,5]]\n",
    "#task_labels = [[0,1], [2,3],[4,5], [6,7],[8,9]]\n",
    "task_labels = [[4,5], [8,9], [6,7], [0,1], [2,3]]\n",
    "#task_labels = [[0,1], [2,3,1,0],[4,5,1,2], [6,7,3,0],[8,9,4,6]]\n",
    "n_tasks = len(task_labels)\n",
    "nb_classes  = 10\n",
    "training_datasets = []\n",
    "validation_datasets = []\n",
    "multihead=False\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(Y_train_all_attacks, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = X_train[idx], np_utils.to_categorical(label_map[Y_train_all_attacks[idx]], len(labels))\n",
    "    else:\n",
    "        data = X_train[idx], np_utils.to_categorical(Y_train_all_attacks[idx], nb_classes)\n",
    "        training_datasets.append(data)\n",
    "\n",
    "for labels in task_labels:\n",
    "    idx = np.in1d(Y_test_all_attacks, labels)\n",
    "    if multihead:\n",
    "        label_map = np.arange(nb_classes)\n",
    "        label_map[labels] = np.arange(len(labels))\n",
    "        data = X_test[idx], np_utils.to_categorical(label_map[Y_test_all_attacks[idx]], len(labels))\n",
    "    else:\n",
    "        data = X_test[idx], np_utils.to_categorical(Y_test_all_attacks[idx], nb_classes)\n",
    "        validation_datasets.append(data)\n",
    "        \n",
    "tasks_train={}; labels_train = {}; tasks_test = {}; labels_test = {}\n",
    "\n",
    "for i in range(len(task_labels)):\n",
    "    tasks_train[str(i)] = training_datasets[i][0]\n",
    "    labels_train[str(i)] = training_datasets[i][1]\n",
    "    tasks_test[str(i)] = validation_datasets[i][0]\n",
    "    labels_test[str(i)] = validation_datasets[i][1]\n",
    "    print('Task {0} size: Trainset - {1}, {2}, Testset - {3}, {4}'.format(i,tasks_train[str(i)].shape, labels_train[str(i)].shape, tasks_test[str(i)].shape, labels_test[str(i)].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check for train set. Each row must has only one 1.\n",
      "Sanity check for test set. Each row must has only one 1.\n",
      "Sanity check for train set. Each task has 1 at right location.\n",
      "Sanity check for test set. Each task has 1 at right location.\n"
     ]
    }
   ],
   "source": [
    "#Task labels sanity check for train set. Each row must has only one '1' \n",
    "print('Sanity check for train set. Each row must has only one 1.')\n",
    "for i in range(len(labels_train)):\n",
    "    for j in range(len(labels_train[str(i)])):\n",
    "        if sum(labels_train[str(i)][j]) > 1 or sum(labels_train[str(i)][j]) == 0:\n",
    "            print(i,j) \n",
    "\n",
    "#Task labels sanity check for test set. Each row must has only one '1' \n",
    "print('Sanity check for test set. Each row must has only one 1.')\n",
    "for i in range(len(labels_test)):\n",
    "    for j in range(len(labels_test[str(i)])):\n",
    "        if sum(labels_test[str(i)][j]) > 1 or sum(labels_test[str(i)][j]) == 0:\n",
    "            print(i,j)\n",
    "\n",
    "#Task labels sanity check for train set. Each task label must has 1 at right location\n",
    "print('Sanity check for train set. Each task has 1 at right location.')\n",
    "for i in range(len(labels_train)):\n",
    "    for j in range(len(labels_train[str(i)])):\n",
    "        if sum(labels_train[str(i)][j][task_labels[i]]) != 1:\n",
    "            print(i,j)\n",
    "           \n",
    "            \n",
    "#Task labels sanity check for test set. Each task must has a non zero label\n",
    "print('Sanity check for test set. Each task has 1 at right location.')\n",
    "for i in range(len(labels_test)):\n",
    "    for j in range(len(labels_test[str(i)])):\n",
    "        if sum(labels_test[str(i)][j][task_labels[i]]) != 1:\n",
    "            print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from OWMLayer_3Layers import OWMLayer\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ignore warning\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # use gpu\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 3, \"Number of checkpoints to store (default: 5)\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.app.flags.DEFINE_string(\"buckets\", \"\", \"\")\n",
    "tf.app.flags.DEFINE_string(\"checkpointDir\", \"\", \"oss info\")\n",
    "tf.flags.DEFINE_integer(\"num_class\", 10, \"\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"epoch\", 50, \"\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pprint\n",
    "def next_batch(l, n,m): \n",
    "    final = [l[i * n:(i + 1) * n,:] for i in range((len(l) + n - 1) // n )]\n",
    "    #Repeating the frazme segmentation for different epochs\n",
    "    if m >= len(final): \n",
    "        m = m % len(final)\n",
    "        if m == 0:\n",
    "            np.random.shuffle(l)\n",
    "            final = [l[i * n:(i + 1) * n,:] for i in range((len(l) + n - 1) // n )]\n",
    "    return(final[m])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Disjoint UNSW Task 1\n",
      "Task performs classification between {4, 5}\n",
      "Eval->>> Task: [1/5] Step: [3/50] acc: 68.7474  %\n",
      "Eval->>> Task: [1/5] Step: [5/50] acc: 73.876  %\n",
      "Eval->>> Task: [1/5] Step: [8/50] acc: 77.3632  %\n",
      "Eval->>> Task: [1/5] Step: [10/50] acc: 81.3832  %\n",
      "Eval->>> Task: [1/5] Step: [13/50] acc: 79.8553  %\n",
      "Eval->>> Task: [1/5] Step: [15/50] acc: 81.3677  %\n",
      "Eval->>> Task: [1/5] Step: [18/50] acc: 80.4482  %\n",
      "Eval->>> Task: [1/5] Step: [20/50] acc: 78.7467  %\n",
      "Eval->>> Task: [1/5] Step: [23/50] acc: 78.36  %\n",
      "Eval->>> Task: [1/5] Step: [25/50] acc: 78.9186  %\n",
      "Train->>>Task: [1/5] Step: 26/50 Train: loss: 0.57, acc: 78.12  %\n",
      "Eval->>> Task: [1/5] Step: [28/50] acc: 78.6385  %\n",
      "Eval->>> Task: [1/5] Step: [30/50] acc: 80.2643  %\n",
      "Eval->>> Task: [1/5] Step: [33/50] acc: 33.2377  %\n",
      "Eval->>> Task: [1/5] Step: [35/50] acc: 22.2415  %\n",
      "Eval->>> Task: [1/5] Step: [38/50] acc: 21.3254  %\n",
      "Eval->>> Task: [1/5] Step: [40/50] acc: 21.994  %\n",
      "Eval->>> Task: [1/5] Step: [43/50] acc: 21.5231  %\n",
      "Eval->>> Task: [1/5] Step: [45/50] acc: 21.2034  %\n",
      "Eval->>> Task: [1/5] Step: [48/50] acc: 21.1708  %\n",
      "Eval->>> Task: [1/5] Step: [50/50] acc: 21.0195  %\n",
      "Train->>>Task: [1/5] Step: 51/50 Train: loss: 0.60, acc: 75.00  %\n",
      "Shape of matrix P1 = (44, 44), P2 = (61, 61) and P3 = (31, 31)\n",
      "Rank of P1 = 43, P2= 60 and P3=30\n",
      "Test on Previous Datasets:\n",
      "{'Task 1': 0.20995462}\n",
      "Aggregate Test Accuracy:->>>[1/5], acc: 21.00 %\n",
      "Training Disjoint UNSW Task 2\n",
      "Task performs classification between {8, 9}\n",
      "Eval->>> Task: [2/5] Step: [3/50] acc: 0  %\n",
      "Eval->>> Task: [2/5] Step: [5/50] acc: 0  %\n",
      "Eval->>> Task: [2/5] Step: [8/50] acc: 0  %\n",
      "Eval->>> Task: [2/5] Step: [10/50] acc: 0  %\n",
      "Eval->>> Task: [2/5] Step: [13/50] acc: 0  %\n",
      "Eval->>> Task: [2/5] Step: [15/50] acc: 0.0266951  %\n",
      "Eval->>> Task: [2/5] Step: [18/50] acc: 0.0266951  %\n",
      "Eval->>> Task: [2/5] Step: [20/50] acc: 0.0533903  %\n",
      "Eval->>> Task: [2/5] Step: [23/50] acc: 0.0533903  %\n",
      "Eval->>> Task: [2/5] Step: [25/50] acc: 33.3422  %\n",
      "Train->>>Task: [2/5] Step: 26/50 Train: loss: 4.60, acc: 0.00  %\n",
      "Eval->>> Task: [2/5] Step: [28/50] acc: 33.3422  %\n",
      "Eval->>> Task: [2/5] Step: [30/50] acc: 33.3689  %\n",
      "Eval->>> Task: [2/5] Step: [33/50] acc: 33.3689  %\n",
      "Eval->>> Task: [2/5] Step: [35/50] acc: 34.6503  %\n",
      "Eval->>> Task: [2/5] Step: [38/50] acc: 35.7982  %\n",
      "Eval->>> Task: [2/5] Step: [40/50] acc: 38.4143  %\n",
      "Eval->>> Task: [2/5] Step: [43/50] acc: 40.1762  %\n",
      "Eval->>> Task: [2/5] Step: [45/50] acc: 41.0838  %\n",
      "Eval->>> Task: [2/5] Step: [48/50] acc: 41.5376  %\n",
      "Eval->>> Task: [2/5] Step: [50/50] acc: 41.7245  %\n",
      "Train->>>Task: [2/5] Step: 51/50 Train: loss: 3.46, acc: 1.56  %\n",
      "Shape of matrix P1 = (44, 44), P2 = (61, 61) and P3 = (31, 31)\n",
      "Rank of P1 = 43, P2= 60 and P3=30\n",
      "Test on Previous Datasets:\n",
      "{'Task 1': 0.003952977, 'Task 2': 0.41724506}\n",
      "Aggregate Test Accuracy:->>>[2/5], acc: 21.06 %\n",
      "Training Disjoint UNSW Task 3\n",
      "Task performs classification between {6, 7}\n",
      "Eval->>> Task: [3/5] Step: [3/50] acc: 43.3121  %\n",
      "Eval->>> Task: [3/5] Step: [5/50] acc: 68.789  %\n",
      "Eval->>> Task: [3/5] Step: [8/50] acc: 69.606  %\n",
      "Eval->>> Task: [3/5] Step: [10/50] acc: 72.8651  %\n",
      "Eval->>> Task: [3/5] Step: [13/50] acc: 73.0249  %\n",
      "Eval->>> Task: [3/5] Step: [15/50] acc: 73.0337  %\n",
      "Eval->>> Task: [3/5] Step: [18/50] acc: 63.3112  %\n",
      "Eval->>> Task: [3/5] Step: [20/50] acc: 53.6522  %\n",
      "Eval->>> Task: [3/5] Step: [23/50] acc: 51.8278  %\n",
      "Eval->>> Task: [3/5] Step: [25/50] acc: 49.6507  %\n",
      "Train->>>Task: [3/5] Step: 25/50 Train: loss: 0.95, acc: 71.88  %\n",
      "Eval->>> Task: [3/5] Step: [28/50] acc: 49.1579  %\n",
      "Eval->>> Task: [3/5] Step: [30/50] acc: 47.3378  %\n",
      "Eval->>> Task: [3/5] Step: [33/50] acc: 44.8277  %\n",
      "Eval->>> Task: [3/5] Step: [35/50] acc: 44.7226  %\n",
      "Eval->>> Task: [3/5] Step: [38/50] acc: 44.8431  %\n",
      "Eval->>> Task: [3/5] Step: [40/50] acc: 45.2242  %\n",
      "Eval->>> Task: [3/5] Step: [43/50] acc: 45.0665  %\n",
      "Eval->>> Task: [3/5] Step: [45/50] acc: 44.3722  %\n",
      "Eval->>> Task: [3/5] Step: [48/50] acc: 44.4072  %\n",
      "Eval->>> Task: [3/5] Step: [50/50] acc: 43.9232  %\n",
      "Train->>>Task: [3/5] Step: 50/50 Train: loss: 0.80, acc: 64.06  %\n",
      "Shape of matrix P1 = (44, 44), P2 = (61, 61) and P3 = (31, 31)\n",
      "Rank of P1 = 42, P2= 60 and P3=30\n",
      "Test on Previous Datasets:\n",
      "{'Task 1': 0.0, 'Task 2': 0.00026695142, 'Task 3': 0.43914405}\n",
      "Aggregate Test Accuracy:->>>[3/5], acc: 14.65 %\n",
      "Training Disjoint UNSW Task 4\n",
      "Task performs classification between {0, 1}\n",
      "Eval->>> Task: [4/5] Step: [3/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [5/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [8/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [10/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [13/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [15/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [18/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [20/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [22/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [25/50] acc: 0  %\n",
      "Train->>>Task: [4/5] Step: 25/50 Train: loss: 4.57, acc: 0.00  %\n",
      "Eval->>> Task: [4/5] Step: [27/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [30/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [32/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [35/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [37/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [39/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [42/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [44/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [47/50] acc: 0  %\n",
      "Eval->>> Task: [4/5] Step: [49/50] acc: 0  %\n",
      "Train->>>Task: [4/5] Step: 50/50 Train: loss: 4.01, acc: 0.00  %\n",
      "Shape of matrix P1 = (44, 44), P2 = (61, 61) and P3 = (31, 31)\n",
      "Rank of P1 = 42, P2= 60 and P3=30\n",
      "Test on Previous Datasets:\n",
      "{'Task 1': 0.0, 'Task 2': 0.35851574, 'Task 3': 0.25674048, 'Task 4': 0.0}\n",
      "Aggregate Test Accuracy:->>>[4/5], acc: 15.38 %\n",
      "Training Disjoint UNSW Task 5\n",
      "Task performs classification between {2, 3}\n",
      "Eval->>> Task: [5/5] Step: [3/50] acc: 11.0045  %\n",
      "Eval->>> Task: [5/5] Step: [5/50] acc: 22.3098  %\n",
      "Eval->>> Task: [5/5] Step: [8/50] acc: 22.1624  %\n",
      "Eval->>> Task: [5/5] Step: [10/50] acc: 29.9529  %\n",
      "Eval->>> Task: [5/5] Step: [13/50] acc: 36.2951  %\n",
      "Eval->>> Task: [5/5] Step: [15/50] acc: 42.1741  %\n",
      "Eval->>> Task: [5/5] Step: [18/50] acc: 71.351  %\n",
      "Eval->>> Task: [5/5] Step: [20/50] acc: 75.6283  %\n",
      "Eval->>> Task: [5/5] Step: [23/50] acc: 77.1593  %\n",
      "Eval->>> Task: [5/5] Step: [25/50] acc: 78.3204  %\n",
      "Train->>>Task: [5/5] Step: 25/50 Train: loss: 0.30, acc: 95.31  %\n",
      "Eval->>> Task: [5/5] Step: [28/50] acc: 78.9746  %\n",
      "Eval->>> Task: [5/5] Step: [30/50] acc: 79.1115  %\n",
      "Eval->>> Task: [5/5] Step: [33/50] acc: 79.1536  %\n",
      "Eval->>> Task: [5/5] Step: [35/50] acc: 79.1415  %\n",
      "Eval->>> Task: [5/5] Step: [38/50] acc: 79.1656  %\n",
      "Eval->>> Task: [5/5] Step: [40/50] acc: 79.1596  %\n",
      "Eval->>> Task: [5/5] Step: [43/50] acc: 79.1596  %\n",
      "Eval->>> Task: [5/5] Step: [45/50] acc: 79.1626  %\n",
      "Eval->>> Task: [5/5] Step: [48/50] acc: 79.1551  %\n",
      "Eval->>> Task: [5/5] Step: [50/50] acc: 79.1551  %\n",
      "Train->>>Task: [5/5] Step: 50/50 Train: loss: 0.42, acc: 93.75  %\n",
      "Shape of matrix P1 = (44, 44), P2 = (61, 61) and P3 = (31, 31)\n",
      "Rank of P1 = 41, P2= 60 and P3=30\n",
      "Test on Previous Datasets:\n",
      "{'Task 1': 0.0,\n",
      " 'Task 2': 0.0,\n",
      " 'Task 3': 0.19271962,\n",
      " 'Task 4': 0.0,\n",
      " 'Task 5': 0.79155076}\n",
      "Aggregate Test Accuracy:->>>[5/5], acc: 19.69 %\n",
      "accu_owm 15.3535 %\n",
      "\n",
      "Aggregate test accuracy of trained tasks after completing each task trainng: [[20.99546194076538, {'Task 1': 0.20995462}], [21.059902012348175, {'Task 2': 0.41724506, 'Task 1': 0.003952977}], [14.64703232049942, {'Task 2': 0.00026695142, 'Task 1': 0.0, 'Task 3': 0.43914405}], [15.381404757499695, {'Task 2': 0.35851574, 'Task 1': 0.0, 'Task 4': 0.0, 'Task 3': 0.25674048}], [19.68540847301483, {'Task 2': 0.0, 'Task 1': 0.0, 'Task 4': 0.0, 'Task 5': 0.79155076, 'Task 3': 0.19271962}]]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def train(Task_list):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "    g1 = tf.Graph()\n",
    "    middle1 = 60\n",
    "    middle2 = 30\n",
    "    with g1.as_default():\n",
    "        OWM = OWMLayer([[43 + 1, middle1], [middle1 + 1, middle2], [middle2 + 1, 10]], seed_num=79)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    test_array = []\n",
    "    with tf.Session(graph=g1, config=config) as sess1:\n",
    "        # Initialize all variables\n",
    "        init = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        sess1.run(init)\n",
    "        task_num = 5\n",
    "        for j in range(0, task_num):\n",
    "            print(\"Training Disjoint UNSW Task %d\" % (j + 1))\n",
    "            # Update the parameters\n",
    "\n",
    "            # Update the parameters\n",
    "            epoch_owm = FLAGS.epoch\n",
    "            batch_size_owm = FLAGS.batch_size\n",
    "            #print('Task performs classification between {0} and rest.'.format(set(np.argmax(mnist_list[j].train.labels,axis=1))))\n",
    "            print('Task performs classification between {0}'.format(set(np.argmax(Task_list[1][str(j)],axis=1))))\n",
    "            all_data = len(Task_list[1][str(j)][:])\n",
    "            all_step = all_data*epoch_owm//batch_size_owm\n",
    "            for current_step in range(all_step):\n",
    "                lamda = current_step/all_step\n",
    "                current_step = current_step+1\n",
    "                batch_xs = next_batch(Task_list[0][str(j)][:], batch_size_owm, current_step)\n",
    "                batch_ys = next_batch(Task_list[1][str(j)][:], batch_size_owm, current_step)\n",
    "                feed_dict = {\n",
    "                    OWM.input_x: batch_xs,\n",
    "                    OWM.input_y: batch_ys,\n",
    "                    OWM.lr_array: np.array([[0.02]]),\n",
    "                    OWM.alpha_array: np.array([[0.9 * 0.001 ** lamda, 1.0 * 0.1 ** lamda, 0.6]]),\n",
    "                }\n",
    "                acc, loss,  _, = sess1.run([OWM.accuracy, OWM.loss, OWM.back_forward], feed_dict,)\n",
    "                if current_step % (all_step // 2) == 0:\n",
    "                    print(\"Train->>>Task: [{:d}/{:d}] Step: {:d}/{:d} Train: loss: {:.2f}, acc: {:.2f}  %\"\n",
    "                          .format(j+1, task_num, current_step*epoch_owm // all_step+1,\n",
    "                                  epoch_owm, loss, acc * 100))\n",
    "                if current_step % (all_step // 20) == 0:\n",
    "                    feed_dict = {\n",
    "                        OWM.input_x: Task_list[2][str(j)][:],\n",
    "                        OWM.input_y: Task_list[3][str(j)][:],\n",
    "                    }\n",
    "                    acc, loss = sess1.run([OWM.accuracy, OWM.loss], feed_dict)\n",
    "                    print(\"Eval->>> Task: [{:d}/{:d}] Step: [{:d}/{:d}] acc: {:g}  %\"\n",
    "                          .format(j+1, task_num, current_step*epoch_owm // all_step+1, epoch_owm, acc * 100))\n",
    "            P1_mat = sess1.run(OWM.P1, feed_dict,)\n",
    "            P2_mat = sess1.run(OWM.P2, feed_dict,)\n",
    "            P3_mat = sess1.run(OWM.P3, feed_dict,)\n",
    "            print('Shape of matrix P1 = {0}, P2 = {1} and P3 = {2}'.format(P1_mat.shape, P2_mat.shape, P3_mat.shape))\n",
    "            print('Rank of P1 = {0}, P2= {1} and P3={2}'.format(np.linalg.matrix_rank(P1_mat), np.linalg.matrix_rank(P2_mat), np.linalg.matrix_rank(P3_mat)))        \n",
    "            print(\"Test on Previous Datasets:\")\n",
    "            correct = []\n",
    "            Per_task_acc_holder = {}\n",
    "            for i_test in range(j + 1):\n",
    "                feed_dict = {\n",
    "                    OWM.input_x: Task_list[2][str(i_test)][:],\n",
    "                    OWM.input_y: Task_list[3][str(i_test)][:],\n",
    "                }\n",
    "                accu, = sess1.run([OWM.accuracy], feed_dict)\n",
    "                Per_task_acc_holder['Task ' + str(i_test+1)] = accu\n",
    "                correct.append(accu)\n",
    "            pprint.pprint(Per_task_acc_holder)\n",
    "            test_accu = 100 * np.mean(correct)\n",
    "            test_array.append([test_accu,Per_task_acc_holder])\n",
    "            print(\"Aggregate Test Accuracy:->>>[{:d}/{:d}], acc: {:.2f} %\".format(j + 1, task_num, test_accu))\n",
    "        feed_dict = {\n",
    "            OWM.input_x: X_test,\n",
    "            OWM.input_y: Y_test_onehot_encoded,\n",
    "        }\n",
    "        accu, loss = sess1.run([OWM.accuracy, OWM.loss], feed_dict)\n",
    "        print(\"accu_owm {:g} %\\n\".format(accu * 100))\n",
    "        print('Aggregate test accuracy of trained tasks after completing each task trainng: {}'.format(test_array))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # Create 10 disjoint MNIST datasets\n",
    "    Task_list = [tasks_train, labels_train, tasks_test, labels_test]\n",
    "    train(Task_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
