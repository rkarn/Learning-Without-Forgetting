{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-5892d0f0cae4>:28: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from OWMLayer_2Layers import OWMLayer\n",
    "import os\n",
    "import pdb\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # ignore warning\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # use gpu\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 3, \"Number of checkpoints to store (default: 5)\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.app.flags.DEFINE_string(\"buckets\", \"\", \"\")\n",
    "tf.app.flags.DEFINE_string(\"checkpointDir\", \"\", \"oss info\")\n",
    "tf.flags.DEFINE_integer(\"num_class\", 10, \"\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 40, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"epoch\", 20, \"\")\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# ==================================================\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def next_batch(l, n,m): \n",
    "    final = [l[i * n:(i + 1) * n,:] for i in range((len(l) + n - 1) // n )]\n",
    "    #Repeating the frazme segmentation for different epochs\n",
    "    if m >= len(final): \n",
    "        m = m % len(final)\n",
    "        if m == 0:\n",
    "            np.random.shuffle(l)\n",
    "            final = [l[i * n:(i + 1) * n,:] for i in range((len(l) + n - 1) // n )]\n",
    "    return(final[m]) \n",
    "\n",
    "#Heuristic metric: Weights Pattern after training each task\n",
    "from math import*\n",
    "#1. Euclidean distance\n",
    "def euclidean_distance(x,y):\n",
    "    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "#2. Cosine Similarity\n",
    "def square_rooted(x):\n",
    "    return round(sqrt(sum([a*a for a in x])),3)\n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return round(numerator/float(denominator),3)\n",
    "\n",
    "#3. Jaccard similarity\n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "#4. Using SequenceMatcher    \n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, pprint\n",
    "from numpy import count_nonzero\n",
    "def split_mnist(mnist, cond):\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "    sets_list = []\n",
    "    for set_name in sets:\n",
    "        this_set = getattr(mnist, set_name)\n",
    "        maxlabels = np.argmax(this_set.labels, 1)\n",
    "        sets_list.append(DataSet(this_set.images[cond(maxlabels),:], this_set.labels[cond(maxlabels)],\n",
    "                                 dtype=dtypes.uint8, reshape=False))\n",
    "    return base.Datasets(train=sets_list[0], validation=sets_list[1], test=sets_list[2])\n",
    "\n",
    "\n",
    "def train(mnist_list):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "    g1 = tf.Graph()\n",
    "    middle = 50\n",
    "    with g1.as_default():\n",
    "        OWM = OWMLayer([[784 + 1, middle], [middle + 1, 10]], seed_num=79)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    test_array = []\n",
    "    Gradients = []\n",
    "    Weights = []\n",
    "    Tasks_Accuracy = []\n",
    "    loss_after_task = []\n",
    "    with tf.Session(graph=g1, config=config) as sess1:\n",
    "        # Initialize all variables\n",
    "        init = [tf.global_variables_initializer(), tf.local_variables_initializer()]\n",
    "        sess1.run(init)\n",
    "        task_num = len(mnist_list)\n",
    "        for j in range(0, task_num):\n",
    "            print(\"Training Disjoint MNIST %d\" % (j + 1))\n",
    "\n",
    "            # Update the parameters\n",
    "            epoch_owm = FLAGS.epoch\n",
    "            batch_size_owm = FLAGS.batch_size\n",
    "            print('Task performs classification between {0}'.format(set(np.argmax(mnist_list[j].train.labels,axis=1))))\n",
    "            all_data = len(mnist_list[j].train.labels[:])\n",
    "            all_step = all_data*epoch_owm//batch_size_owm\n",
    "            for current_step in range(all_step):\n",
    "                lamda = current_step/all_step\n",
    "                current_step = current_step+1\n",
    "                batch_xs, batch_ys = mnist_list[j].train.next_batch(batch_size_owm)\n",
    "                feed_dict = {\n",
    "                    OWM.input_x: batch_xs,\n",
    "                    OWM.input_y: batch_ys,\n",
    "                    OWM.lr_array: np.array([[0.2]]),\n",
    "                    OWM.alpha_array: np.array([[0.9 * 0.001 ** lamda, 0.6]]),\n",
    "                }\n",
    "                acc, loss, grads_var, _, = sess1.run([OWM.accuracy, OWM.loss, OWM.grads , OWM.back_forward], feed_dict,)\n",
    "                '''\n",
    "                if current_step % (all_step // 4) == 0:\n",
    "                    print(\"Train->>>Task: [{:d}/{:d}] Step: {:d}/{:d} Train: loss: {:.2f}, acc: {:.2f}  %\"\n",
    "                          .format(j+1, task_num,current_step*epoch_owm // all_step+1, epoch_owm, loss, acc * 100))\n",
    "                '''\n",
    "            print('Shape of P1 = {}. Rank of P1 = {}. Shape of P2 = {}. Rank of P2={}'.format(sess1.run(OWM.P1, feed_dict,).shape, np.linalg.matrix_rank(sess1.run(OWM.P1, feed_dict,)), sess1.run(OWM.P2, feed_dict,).shape, np.linalg.matrix_rank(sess1.run(OWM.P2, feed_dict,))))\n",
    "            print('Number of non-zero in P1 = {}, in P2={}'.format(count_nonzero(sess1.run(OWM.P1, feed_dict,)), count_nonzero(sess1.run(OWM.P2, feed_dict,))))\n",
    "            \n",
    "            loss_after_task.append(loss)\n",
    "            Gradients.append([grads_var[0][0], grads_var[0][1]])\n",
    "            Weights.append([grads_var[1][0], grads_var[1][1]])  \n",
    "            print(\"Test on Previous Datasets:\")\n",
    "            correct = []\n",
    "            Per_task_acc_holder = {}\n",
    "            for i_test in range(task_num):\n",
    "                feed_dict = {\n",
    "                    OWM.input_x: mnist_list[i_test].test.images[:],\n",
    "                    OWM.input_y: mnist_list[i_test].test.labels[:],\n",
    "                }\n",
    "                accu, = sess1.run([OWM.accuracy], feed_dict)\n",
    "                Per_task_acc_holder['Task' + str(i_test+1)] = accu\n",
    "                correct.append(accu)\n",
    "            pprint.pprint(Per_task_acc_holder)\n",
    "            Tasks_Accuracy.append(correct)\n",
    "            test_accu = np.mean(correct)\n",
    "            test_array.append(test_accu)\n",
    "    return(Gradients,Weights, Tasks_Accuracy, loss_after_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /root/OWM/Disjoint MNIST/OWM/OWMLayer_2Layers.py:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Training Disjoint MNIST 1\n",
      "Task performs classification between {0, 1}\n",
      "Shape of P1 = (785, 785). Rank of P1 = 769. Shape of P2 = (51, 51). Rank of P2=50\n",
      "Number of non-zero in P1 = 373495, in P2=2601\n",
      "Test on Previous Datasets:\n",
      "{'Task1': 0.9990544, 'Task2': 0.0, 'Task3': 0.0, 'Task4': 0.0, 'Task5': 0.0}\n",
      "Training Disjoint MNIST 2\n",
      "Task performs classification between {2, 3}\n",
      "Shape of P1 = (785, 785). Rank of P1 = 749. Shape of P2 = (51, 51). Rank of P2=50\n",
      "Number of non-zero in P1 = 447677, in P2=2601\n",
      "Test on Previous Datasets:\n",
      "{'Task1': 0.99101657,\n",
      " 'Task2': 0.94074434,\n",
      " 'Task3': 0.0,\n",
      " 'Task4': 0.0,\n",
      " 'Task5': 0.0}\n",
      "Training Disjoint MNIST 3\n",
      "Task performs classification between {4, 5}\n",
      "Shape of P1 = (785, 785). Rank of P1 = 734. Shape of P2 = (51, 51). Rank of P2=50\n",
      "Number of non-zero in P1 = 467957, in P2=2601\n",
      "Test on Previous Datasets:\n",
      "{'Task1': 0.98345155,\n",
      " 'Task2': 0.89177275,\n",
      " 'Task3': 0.8991462,\n",
      " 'Task4': 0.0,\n",
      " 'Task5': 0.0}\n",
      "Training Disjoint MNIST 4\n",
      "Task performs classification between {6, 7}\n",
      "Shape of P1 = (785, 785). Rank of P1 = 724. Shape of P2 = (51, 51). Rank of P2=50\n",
      "Number of non-zero in P1 = 512725, in P2=2601\n",
      "Test on Previous Datasets:\n",
      "{'Task1': 0.97966903,\n",
      " 'Task2': 0.8687561,\n",
      " 'Task3': 0.86125934,\n",
      " 'Task4': 0.9144008,\n",
      " 'Task5': 0.0}\n",
      "Training Disjoint MNIST 5\n",
      "Task performs classification between {8, 9}\n",
      "Shape of P1 = (785, 785). Rank of P1 = 713. Shape of P2 = (51, 51). Rank of P2=50\n",
      "Number of non-zero in P1 = 512725, in P2=2601\n",
      "Test on Previous Datasets:\n",
      "{'Task1': 0.9716312,\n",
      " 'Task2': 0.86043096,\n",
      " 'Task3': 0.7886873,\n",
      " 'Task4': 0.86858004,\n",
      " 'Task5': 0.70146245}\n",
      "Tasks_Accuracy = [[0.9990544, 0.0, 0.0, 0.0, 0.0], [0.99101657, 0.94074434, 0.0, 0.0, 0.0], [0.98345155, 0.89177275, 0.8991462, 0.0, 0.0], [0.97966903, 0.8687561, 0.86125934, 0.9144008, 0.0], [0.9716312, 0.86043096, 0.7886873, 0.86858004, 0.70146245]], loss_after_task = [8.791544e-07, 0.05458253, 0.4914177, 0.45470268, 1.4289747]\n",
      "[[0.9990544, 0.0, 0.0, 0.0, 0.0], [0.99101657, 0.94074434, 0.0, 0.0, 0.0], [0.98345155, 0.89177275, 0.8991462, 0.0, 0.0], [0.97966903, 0.8687561, 0.86125934, 0.9144008, 0.0], [0.9716312, 0.86043096, 0.7886873, 0.86858004, 0.70146245]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display\n",
    "def main(_):\n",
    "    # Create 2 disjoint MNIST datasets\n",
    "    dataset01 = split_mnist(mnist, lambda x: x < 2)\n",
    "    dataset23 = split_mnist(mnist, lambda x: (1 < x) & (x < 4))\n",
    "    dataset45 = split_mnist(mnist, lambda x: (3 < x) & (x < 6))\n",
    "    dataset67 = split_mnist(mnist, lambda x: (5 < x) & (x < 8))\n",
    "    dataset89 = split_mnist(mnist, lambda x: (7 < x) & (x < 10))\n",
    "    mnist_list = [dataset01, dataset23, dataset45, dataset67, dataset89]\n",
    "    for dtask in mnist_list:\n",
    "        print('Rank of the training task {} is {}'.format(i,np.linalg.matrix_rank(dtask)))\n",
    "    Gradients, Weights, Tasks_Accuracy,loss_after_task = train(mnist_list)\n",
    "    print('Tasks_Accuracy = {}, loss_after_task = {}'.format(Tasks_Accuracy, loss_after_task))\n",
    "    \n",
    "    import pdb\n",
    "    import seaborn as sns\n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(figsize=(4, 4.5))\n",
    "    axs = [plt.subplot(1,1,1)]#, None, None]\n",
    "    fontsize = 13\n",
    "    average_acc=[]\n",
    "    for i in range(len(Tasks_Accuracy)):\n",
    "        average_acc.append(np.mean(Tasks_Accuracy[i]))\n",
    "    \n",
    "    print(Tasks_Accuracy)\n",
    "    colorVal = ['r','g','b','c','m','k','y','tomato','m','c']\n",
    "    marker=[\"v\",\"^\",\"<\",\">\",\"*\",\"s\",\"h\",\"x\",\".\",\"+\"]\n",
    "    n_tasks = 5\n",
    "    mean_vals = np.array(Tasks_Accuracy)\n",
    "    for j in range(n_tasks):\n",
    "        label = \"$\\mathcal{T}$=%g\"%(j+1)\n",
    "        axs[0].plot(range(1,n_tasks+1), mean_vals[:, j], marker=marker[j], c=colorVal[j], label=label)\n",
    "    axs[0].plot(range(1,n_tasks+1), average_acc, marker=marker[j+1], c=colorVal[j+1], label='Average')\n",
    "    axs[0].legend(loc='upper right',prop={'size': fontsize}, bbox_to_anchor=(1.05, -0.25), ncol=3)\n",
    "    axs[0].set_xlabel(\"Trained tasks($\\mathcal{T}$).\",fontsize=fontsize)\n",
    "    axs[0].set_ylabel('Accuracy(x100%).',fontsize=fontsize)\n",
    "    axs[0].set_xticks(range(1,n_tasks+1))\n",
    "    axs[0].set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    axs[0].tick_params(labelsize='large', width=3)\n",
    "    axs[0].set_title('OWM for AWID dataset', fontsize=fontsize)\n",
    "    #plt.legend(bbox_to_anchor=(0.78, 0.66),bbox_transform=plt.gcf().transFigure)\n",
    "    plt.gcf().tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig('AWID_OWM.pdf',bbox_inches = \"tight\")\n",
    "    \n",
    "    \n",
    "    def flatten(x):\n",
    "        try:\n",
    "            it = iter(x)\n",
    "        except TypeError:\n",
    "            yield x\n",
    "        else:\n",
    "            for i in it:\n",
    "                for j in flatten(i):\n",
    "                    yield j\n",
    "    Flatten_weights=[]\n",
    "    for i in range(len(Gradients)):\n",
    "        Flatten_weights.append(list(flatten(Gradients[i]))) \n",
    "    \n",
    "    #Creating the Hessian matrix from the gradients\n",
    "    import math\n",
    "    Hessian_matrix = []\n",
    "    Hessian_row = []\n",
    "    Rank_Hessian_matrix = []\n",
    "    for k in range(len(Flatten_weights)):\n",
    "        for i in range(len(Flatten_weights[k])):\n",
    "            partial_deriv_1 = (math.ceil(Flatten_weights[k][i]*1e10)/1e10)\n",
    "            for j in range(len(Flatten_weights[k])):\n",
    "                partial_deriv_2 = (math.ceil(Flatten_weights[k][j]*1e10)/1e10)\n",
    "                Hessian_row.append(np.around(partial_deriv_1*partial_deriv_2))\n",
    "            Hessian_matrix.append(Hessian_row)\n",
    "            Hessian_row = []\n",
    "        rank_Hessian_matrix = np.linalg.matrix_rank(np.asarray(Hessian_matrix))\n",
    "        Rank_Hessian_matrix.append(rank_Hessian_matrix)\n",
    "        print('Rank of the Hessian Matrix after task=',k,'is :',rank_Hessian_matrix)  \n",
    "    \n",
    "    #Rank_Hessian_matrix = [22, 53, 101, 157, 268]  #Calculated by uncommenting above code and then coped here\n",
    "    \n",
    "    figure(figsize=(3, 5))\n",
    "    axs = [plt.subplot(2,1,1)]#, None, None]\n",
    "    for i in range(1, 2):\n",
    "        axs.append(plt.subplot(2, 1, i+1, sharex=axs[0]))\n",
    "    fontsize = 13\n",
    "\n",
    "    colorVal = ['r','g','b','c','m','k','y']\n",
    "    marker=[\"v\",\"^\",\"<\",\">\",\"*\",\"s\"]\n",
    "    #loss_after_task[-1]=5\n",
    "    plots = [loss_after_task,Rank_Hessian_matrix]\n",
    "    ylabel = ['Task loss ($L_{\\mu+1}$)', 'Hessian rank R(H)']\n",
    "    ytick = [[0,1.0,2.0,3.0,4.0,5.0, 6.0], [0,300,600,900, 1200]]\n",
    "    title = ['Loss', 'Hessian rank']\n",
    "    for i in range(2):\n",
    "        axs[i].plot(range(1,n_tasks+1), plots[i], marker=marker[i], c=colorVal[i])\n",
    "        axs[i].set_ylabel(ylabel[i],fontsize=fontsize)\n",
    "        axs[i].set_yticks(ytick[i])\n",
    "        axs[i].tick_params(labelsize='large', width=3)\n",
    "        axs[i].grid()\n",
    "    axs[-1].set_xticks(range(1,n_tasks+1))\n",
    "    axs[-1].set_xlabel(\"Trained tasks($\\mathcal{T}$).\",fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig('AWID_OWM_evalcongestion.pdf',bbox_inches = \"tight\")\n",
    "    \n",
    "    \n",
    "    model_weights_save=[]\n",
    "    for i in range(len(Weights)):\n",
    "        model_weights_save.append(list(flatten(Weights[i])))\n",
    "        \n",
    "    print(\"--------------Euclidean distance--------------------\")\n",
    "    Eu_distance = []\n",
    "    for i in range(1,n_tasks):\n",
    "        eu_dist = euclidean_distance(list(flatten(model_weights_save[i-1])),list(flatten(model_weights_save[i])))\n",
    "        Eu_distance.append(eu_dist)\n",
    "        print(\"Between task {0} and {1} : {2}\".format(i-1,i,eu_dist))\n",
    "\n",
    "    print(\"--------------Cosine distance--------------------\")\n",
    "    Cos_distance = []\n",
    "    for i in range(1,n_tasks):\n",
    "        cos_dist = cosine_similarity(list(flatten(model_weights_save[i-1])),list(flatten(model_weights_save[i])))\n",
    "        Cos_distance.append(cos_dist)\n",
    "        print(\"Between task {0} and {1} : {2}\".format(i-1,i,cos_dist))\n",
    "\n",
    "\n",
    "    print(\"--------------Jaccard similarity--------------------\")\n",
    "    Jac_distance = []\n",
    "    for i in range(1,n_tasks):\n",
    "        jac_dist = jaccard_similarity(list(flatten(model_weights_save[i-1])),list(flatten(model_weights_save[i])))\n",
    "        Jac_distance.append(jac_dist)\n",
    "        print(\"Between task {0} and {1} : {2}\".format(i-1,i,jac_dist))\n",
    "\n",
    "    print(\"--------------Sequence Matcher--------------------\")\n",
    "    Seq_matcher = []\n",
    "    for i in range(1,n_tasks):\n",
    "        seq_matcher = SequenceMatcher(None,list(flatten(model_weights_save[i-1])),list(flatten(model_weights_save[i]))).ratio()\n",
    "        Seq_matcher.append(seq_matcher)\n",
    "        print(\"Between task {0} and {1} : {2}\".format(i-1,i,seq_matcher))\n",
    "    \n",
    "    figure(figsize=(3, 8))\n",
    "    plt.subplots_adjust(hspace=0.25,\n",
    "                        wspace=0.35)\n",
    "    axs = [plt.subplot(4,1,1)]#, None, None]\n",
    "    for i in range(1, 4):\n",
    "        axs.append(plt.subplot(4, 1, i+1, sharex=axs[0]))\n",
    "    fontsize = 13\n",
    "    colorVal = ['r','g','b','c','m','k','y']\n",
    "    marker=[\"v\",\"^\",\"<\",\">\",\"*\",\"s\"]\n",
    "    plots = [Eu_distance,Cos_distance,Jac_distance,Seq_matcher]\n",
    "    ylabel = ['Distance value', 'Ratio value', 'Ratio value','Ratio value']\n",
    "    ytick = [[0.0, 3.0, 6.0, 9.0, 12.0], [0, 0.5, 1.0],[0, 0.4, 0.8, 1.2], [0, 0.4, 0.8, 1.2]]\n",
    "    title = ['Euclidean Distance', 'Cosine Similarity', 'Jaccard Similarity', 'SequenceMatcher']\n",
    "    for i in range(4):\n",
    "        axs[i].plot(range(2,n_tasks+1), plots[i], marker=marker[i], c=colorVal[i])\n",
    "        axs[i].set_ylabel(ylabel[i],fontsize=fontsize)\n",
    "        axs[i].set_yticks(ytick[i])\n",
    "        axs[i].tick_params(labelsize='large', width=3)\n",
    "        axs[i].grid()\n",
    "        axs[i].set_title(title[i])\n",
    "    axs[-1].set_xticks(range(2,n_tasks+1))\n",
    "    axs[-1].set_xlabel(\"Trained tasks($\\mathcal{T}$).\",fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    sns.despine()\n",
    "    plt.savefig('AWID_OWM_Similaritymetric.pdf',bbox_inches = \"tight\")\n",
    "    \n",
    "    #Heuristic metric: Weights Pattern between task 0 and last task \n",
    "    n_tasks = len(model_weights_save)\n",
    "    eu_dist = euclidean_distance(list(flatten(model_weights_save[0])),list(flatten(model_weights_save[-1])))\n",
    "    print(\"Euclidean distance between task {0} and {1} : {2}\".format(0,len(model_weights_save)-1,eu_dist))\n",
    "\n",
    "    cos_dist = cosine_similarity(list(flatten(model_weights_save[0])),list(flatten(model_weights_save[-1])))\n",
    "    print(\"Cosine angle between task {0} and {1} : {2}\".format(0,len(model_weights_save)-1,cos_dist))\n",
    "\n",
    "    jac_dist = jaccard_similarity(list(flatten(model_weights_save[0])),list(flatten(model_weights_save[-1])))\n",
    "    print(\"Jaccard similarity between task {0} and {1} : {2}\".format(0,len(model_weights_save)-1,jac_dist))\n",
    "\n",
    "    seq_matcher = SequenceMatcher(None,list(flatten(model_weights_save[0])),list(flatten(model_weights_save[-1]))).ratio()\n",
    "    print(\"Sequence angle between task {0} and {1} : {2}\".format(0,len(model_weights_save)-1,seq_matcher)) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
